import cx_Oracle

import pandas as pd

import pandas_gbq

from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud.bigquery import LoadJobConfig, WriteDisposition

# Connection details
username = ###
password = ###
dsn = cx_Orace.makedsn('', Port_Name, sid='')

# Establish the connection
connection = cx_Oracle.connect(username, password, dsn)

# Create a cursor object from the connection
cursor = connection.cursor()

# Execute a query
cursor.execute("SELECT * FROM ECA_ACTIVITY_DATA_V")

# Fetch the results
rows = cursor.fetchall()

# Get column names from cursor description
columns = [col[0] for col in cursor.description]

# Convert to DataFrame
df = pd.DataFrame(rows, columns=columns)

# Don't forget to close the cursor and connection when done
cursor.close()
connection.close()

Branch_Info = pandas_gbq.read_gbq('SELECT * FROM `sturdy-torch-338204.bops_posted_net_strength.unique_branch_info`')
Branch_Info

final_Data = df.merge(Branch_Info,left_on="BRANCH_ID", right_on="Branch_ID", how="left")
final_Data.columns

columns_to_drop = ["REGION_ID","REGION_NAME","BRANCH_ID","BR_NAME","SCHOOL_GROUP_ID","SCHOOL_GROUP_NAME","BR_ABBRV", "CATEGORY_TYPE_ID", "ACT_STATUS_ID","CENTRE_ECA", "NORTH_ECA", "SOUTH_ECA", "SCHOOL_GROUP_ID"]
final_Data.drop(columns=columns_to_drop, inplace=True)
final_Data
final_Data.columns = [col.title() for col in final_Data.columns]
#final_Data["Branch_ID"]
final_Data.columns

final_Data.rename(columns={"Branch_Id": "School_ID", "Bcp_Campuses" : "BCP_Campuses", "Region": "Regions"}, inplace=True)
final_Data

column_Order = columns = [
    'Regions','School_ID', 'Branch_Name','Cluster', 'Old_Cluster_Name',
    'Cities', 'Province', 'BCP_Campuses','Activity_Id', 'Activity_Name', 'Category_Id', 'Category_Name',
    'Start_Date', 'End_Date', 'Total_Students', 'Total_Amount_Received',
    'Total_Expense', 'Balance_Amount', 'Act_Status','Received_Amount_Range', 'Rcvd_Amt_Per_Std', 'Rcvd_Amt_Range_Per_Std',
    'Total_Students_Reg', 'Month', 'Year'
]

final_Data = final_Data[column_Order]

# Provide the path to your service account key file
service_account_path = "D:\Big Query Access.json"

# Authenticate and create a BigQuery client
credentials = service_account.Credentials.from_service_account_file(service_account_path)
client = bigquery.Client(credentials=credentials, project=credentials.project_id)

# Now you can use the client to interact with BigQuery
table_id = 'sturdy-torch-338204.ECA_Dataset.ECA_Activity'
# Create a job configuration to replace the table data
job_config = LoadJobConfig()
job_config.write_disposition = WriteDisposition.WRITE_TRUNCATE

# Load the DataFrame into the BigQuery table, replacing existing data
job = client.load_table_from_dataframe(final_Data, table_id, job_config=job_config)

# Wait for the load job to complete
job.result()

print("Data loaded successfully and existing data replaced.")
#job = client.load_table_from_dataframe(final_Data, table_id)
#job.result()  # Wait for the job to complete
